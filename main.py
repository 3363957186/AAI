import json
import os

import httpx
from config import YOUTUBE_API_KEY, MAX_VIDEOS, MAX_COMMENTS_PER_VIDEO
from storage.database import init_db, save_video, save_comments, get_all_comments
from storage.models import Video, Comment
from preprocess import preprocess_comments
from sentiment import analyze_batch
from storage.database import save_sentiment, get_sentiment_summary, get_comments_by_sentiment

VIDEO_URL    = "https://www.googleapis.com/youtube/v3/videos"
SEARCH_URL   = "https://www.googleapis.com/youtube/v3/search"
THREADS_URL  = "https://www.googleapis.com/youtube/v3/commentThreads"
COMMENTS_URL = "https://www.googleapis.com/youtube/v3/comments"
COMMENTS_DIR = "comments"


# â”€â”€ è§†é¢‘æœç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_video_id_from_url(url: str) -> str:
    if "v=" in url:
        return url.split("v=")[1].split("&")[0]
    if "youtu.be/" in url:
        return url.split("youtu.be/")[1].split("?")[0]
    return url

def _mode_search() -> list[Video]:
    keyword = input("\nè¯·è¾“å…¥å•†å“åç§°ï¼ˆè‹±æ–‡ï¼‰: ").strip()
    if not keyword:
        print("ä¸èƒ½ä¸ºç©º")
        return []
    return search_videos(keyword, max_results=MAX_VIDEOS)


def _mode_url() -> list[Video]:
    print("\nè¯·è¾“å…¥è§†é¢‘é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæœ€å¤š 3 ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
    urls = []
    while len(urls) < MAX_VIDEOS:
        line = input(f"  é“¾æ¥ {len(urls)+1}: ").strip()
        if not line:
            break
        if "youtube.com" in line or "youtu.be" in line:
            urls.append(line)
        else:
            print("  âš ï¸  æ— æ•ˆçš„ YouTube é“¾æ¥ï¼Œè¯·é‡æ–°è¾“å…¥")

    if not urls:
        print("æœªè¾“å…¥ä»»ä½•é“¾æ¥")
        return []

    # æ‰¹é‡è·å–è§†é¢‘ä¿¡æ¯
    video_ids = [get_video_id_from_url(u) for u in urls]
    resp = httpx.get(VIDEO_URL, params={
        "key":  YOUTUBE_API_KEY,
        "id":   ",".join(video_ids),
        "part": "snippet,statistics",
    })
    resp.raise_for_status()

    videos = []
    for item in resp.json().get("items", []):
        stats   = item.get("statistics", {})
        snippet = item.get("snippet", {})
        videos.append(Video(
            video_id      = item["id"],
            author        = snippet.get("channelTitle", ""),
            description   = snippet.get("title", ""),
            view_count    = int(stats.get("viewCount", 0)),
            like_count    = int(stats.get("likeCount", 0)),
            comment_count = int(stats.get("commentCount", 0)),
        ))

    return videos

def search_videos(keyword: str, max_results: int = 2) -> list[Video]:
    print(f"[VideoSearcher] æœç´¢: '{keyword} review'")

    resp = httpx.get(SEARCH_URL, params={
        "key":               YOUTUBE_API_KEY,
        "q":                 f"{keyword} review",
        "part":              "snippet",
        "type":              "video",
        "maxResults":        max_results,
        "relevanceLanguage": "en",
    })
    resp.raise_for_status()
    items = resp.json().get("items", [])
    if not items:
        return []

    video_ids = [item["id"]["videoId"] for item in items]

    stats_resp = httpx.get(VIDEO_URL, params={
        "key":  YOUTUBE_API_KEY,
        "id":   ",".join(video_ids),
        "part": "snippet,statistics",
    })
    stats_resp.raise_for_status()

    videos = []
    for item in stats_resp.json().get("items", []):
        stats   = item.get("statistics", {})
        snippet = item.get("snippet", {})
        videos.append(Video(
            video_id      = item["id"],
            author        = snippet.get("channelTitle", ""),
            description   = snippet.get("title", ""),
            view_count    = int(stats.get("viewCount", 0)),
            like_count    = int(stats.get("likeCount", 0)),
            comment_count = int(stats.get("commentCount", 0)),
        ))

    print(f"[VideoSearcher] æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
    return videos


# â”€â”€ è¯„è®ºæŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_replies(parent_id: str, video_id: str) -> list[Comment]:
    replies = []
    next_page_token = None

    while True:
        params = {
            "key":        YOUTUBE_API_KEY,
            "parentId":   parent_id,
            "part":       "snippet",
            "maxResults": 100,
        }
        if next_page_token:
            params["pageToken"] = next_page_token

        try:
            resp = httpx.get(COMMENTS_URL, params=params, timeout=15)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"    [å›å¤] è¯·æ±‚å¤±è´¥: {e}")
            break

        for item in data.get("items", []):
            s = item["snippet"]
            replies.append(Comment(
                comment_id  = item["id"],
                video_id    = video_id,
                parent_id   = parent_id,
                username    = s.get("authorDisplayName", ""),
                text        = s.get("textOriginal", ""),
                like_count  = s.get("likeCount", 0),
                reply_count = 0,
                created_at  = s.get("publishedAt", ""),
            ))

        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break

    return replies


def fetch_all_comments(video_id: str) -> list[Comment]:
    all_comments = []
    next_page_token = None
    page = 1

    while True:
        params = {
            "key":        YOUTUBE_API_KEY,
            "videoId":    video_id,
            "part":       "snippet,replies",
            "maxResults": 100,
            "order":      "relevance",
        }
        if next_page_token:
            params["pageToken"] = next_page_token

        try:
            resp = httpx.get(THREADS_URL, params=params, timeout=15)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"[ç¬¬ {page} é¡µ] è¯·æ±‚å¤±è´¥: {e}")
            break

        for item in data.get("items", []):
            top        = item["snippet"]["topLevelComment"]["snippet"]
            comment_id = item["snippet"]["topLevelComment"]["id"]
            reply_count = item["snippet"].get("totalReplyCount", 0)

            all_comments.append(Comment(
                comment_id  = comment_id,
                video_id    = video_id,
                parent_id   = None,
                username    = top.get("authorDisplayName", ""),
                text        = top.get("textOriginal", ""),
                like_count  = top.get("likeCount", 0),
                reply_count = reply_count,
                created_at  = top.get("publishedAt", ""),
            ))

            if reply_count > 0:
                embedded = item.get("replies", {}).get("comments", [])
                if reply_count <= len(embedded):
                    for r in embedded:
                        s = r["snippet"]
                        all_comments.append(Comment(
                            comment_id  = r["id"],
                            video_id    = video_id,
                            parent_id   = comment_id,
                            username    = s.get("authorDisplayName", ""),
                            text        = s.get("textOriginal", ""),
                            like_count  = s.get("likeCount", 0),
                            reply_count = 0,
                            created_at  = s.get("publishedAt", ""),
                        ))
                else:
                    print(f"    â””â”€ è·å– {reply_count} æ¡å›å¤...")
                    all_comments.extend(fetch_replies(comment_id, video_id))

        top_count         = sum(1 for c in all_comments if c.parent_id is None)
        reply_count_total = sum(1 for c in all_comments if c.parent_id is not None)
        print(f"  ç¬¬ {page} é¡µ: é¡¶å±‚ {top_count} æ¡ï¼Œå›å¤ {reply_count_total} æ¡")

        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            print("  å·²åˆ°æœ€åä¸€é¡µ")
            break

        page += 1

    return all_comments


# â”€â”€ å¯¼å‡º txt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def export_to_txt(video: Video, all_stored: list[dict]):
    os.makedirs(COMMENTS_DIR, exist_ok=True)
    filename = os.path.join(COMMENTS_DIR, f"{video.video_id}_comments.txt")
    top_level = [c for c in all_stored if c["parent_id"] is None]
    replies   = [c for c in all_stored if c["parent_id"] is not None]

    with open(filename, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write(f"è§†é¢‘æ ‡é¢˜ï¼š{video.description}\n")
        f.write(f"é¢‘é“ï¼š    {video.author}\n")
        f.write(f"é“¾æ¥ï¼š    https://youtube.com/watch?v={video.video_id}\n")
        f.write(f"æ’­æ”¾ï¼š    {video.view_count:,}\n")
        f.write(f"é¡¶å±‚è¯„è®ºï¼š{len(top_level)} æ¡\n")
        f.write(f"å›å¤ï¼š    {len(replies)} æ¡\n")
        f.write(f"æ€»è®¡ï¼š    {len(all_stored)} æ¡\n")
        f.write("=" * 60 + "\n\n")

        for i, c in enumerate(top_level, 1):
            f.write(f"[{i}] {c['username']}  ğŸ‘{c['like_count']}  {c['created_at'][:10]}\n")
            f.write(f"{c['text']}\n")

            comment_replies = [r for r in replies if r["parent_id"] == c["comment_id"]]
            for r in comment_replies:
                f.write(f"\n    â†³ {r['username']}  ğŸ‘{r['like_count']}  {r['created_at'][:10]}\n")
                f.write(f"    {r['text']}\n")

            f.write("\n" + "-" * 60 + "\n\n")

    print(f"  å·²å¯¼å‡ºåˆ° {filename}")

def export_to_txt_v2(video: Video, all_stored: list[dict], summary: dict):
    os.makedirs(COMMENTS_DIR, exist_ok=True)
    filename = os.path.join(COMMENTS_DIR, f"{video.video_id}_comments_v2.txt")
    top_level = [c for c in all_stored if c["parent_id"] is None]
    replies   = [c for c in all_stored if c["parent_id"] is not None]
    total     = sum(summary.values())

    with open(filename, "w", encoding="utf-8") as f:

        # â”€â”€ è§†é¢‘åŸºæœ¬ä¿¡æ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        f.write("=" * 60 + "\n")
        f.write(f"è§†é¢‘æ ‡é¢˜ï¼š{video.description}\n")
        f.write(f"é¢‘é“ï¼š    {video.author}\n")
        f.write(f"é“¾æ¥ï¼š    https://youtube.com/watch?v={video.video_id}\n")
        f.write(f"æ’­æ”¾ï¼š    {video.view_count:,}\n")
        f.write(f"é¡¶å±‚è¯„è®ºï¼š{len(top_level)} æ¡\n")
        f.write(f"å›å¤ï¼š    {len(replies)} æ¡\n")
        f.write(f"æ€»è®¡ï¼š    {len(all_stored)} æ¡\n")
        f.write("=" * 60 + "\n\n")

        # â”€â”€ æƒ…æ„Ÿåˆ†ææ±‡æ€» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        f.write("ã€æƒ…æ„Ÿåˆ†ææ±‡æ€»ã€‘\n")
        f.write("-" * 60 + "\n")
        if total > 0:
            f.write(f"æ­£é¢ (Positive): {summary['positive']} æ¡ "
                    f"({summary['positive']/total*100:.1f}%)\n")
            f.write(f"è´Ÿé¢ (Negative): {summary['negative']} æ¡ "
                    f"({summary['negative']/total*100:.1f}%)\n")
            f.write(f"ä¸­æ€§ (Neutral):  {summary['neutral']}  æ¡ "
                    f"({summary['neutral']/total*100:.1f}%)\n")
        f.write("\n")

        # æœ€æ­£é¢ Top 5
        positives = sorted(
            [c for c in all_stored if c.get("sentiment_label") == "positive"],
            key=lambda x: x.get("sentiment_score", 0), reverse=True
        )
        f.write("Top 5 æœ€æ­£é¢è¯„è®ºï¼š\n")
        for i, c in enumerate(positives[:5], 1):
            f.write(f"  {i}. [{c['sentiment_score']:.2f}] {c['clean_text'][:80]}\n")
        f.write("\n")

        # æœ€è´Ÿé¢ Top 5
        negatives = sorted(
            [c for c in all_stored if c.get("sentiment_label") == "negative"],
            key=lambda x: x.get("sentiment_score", 0), reverse=True
        )
        f.write("Top 5 æœ€è´Ÿé¢è¯„è®ºï¼š\n")
        for i, c in enumerate(negatives[:5], 1):
            f.write(f"  {i}. [{c['sentiment_score']:.2f}] {c['clean_text'][:80]}\n")
        f.write("\n")
        f.write("=" * 60 + "\n\n")

        # â”€â”€ æ‰€æœ‰è¯„è®ºæ­£æ–‡ï¼ˆå¸¦æƒ…æ„Ÿæ ‡ç­¾ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        f.write("ã€æ‰€æœ‰è¯„è®ºã€‘\n\n")
        LABEL_MAP = {"positive": "âœ…", "negative": "âŒ", "neutral": "â–"}

        for i, c in enumerate(top_level, 1):
            label = c.get("sentiment_label", "")
            score = c.get("sentiment_score", 0)
            icon  = LABEL_MAP.get(label, "")

            f.write(f"[{i}] {icon} {c['username']}  "
                    f"ğŸ‘{c['like_count']}  {c['created_at'][:10]}\n")
            f.write(f"åŸæ–‡ï¼š{c['text']}\n")

            # åªæœ‰åšè¿‡æƒ…æ„Ÿåˆ†æçš„æ‰æ˜¾ç¤º
            if label:
                f.write(f"æƒ…æ„Ÿï¼š{label} (ç½®ä¿¡åº¦ {score:.2f})\n")

            # å›å¤
            comment_replies = [r for r in replies
                               if r["parent_id"] == c["comment_id"]]
            for r in comment_replies:
                r_label = r.get("sentiment_label", "")
                r_score = r.get("sentiment_score", 0)
                r_icon  = LABEL_MAP.get(r_label, "")
                f.write(f"\n    â†³ {r_icon} {r['username']}  "
                        f"ğŸ‘{r['like_count']}  {r['created_at'][:10]}\n")
                f.write(f"    åŸæ–‡ï¼š{r['text']}\n")
                if r_label:
                    f.write(f"    æƒ…æ„Ÿï¼š{r_label} (ç½®ä¿¡åº¦ {r_score:.2f})\n")

            f.write("\n" + "-" * 60 + "\n\n")

    print(f"  å·²å¯¼å‡ºåˆ° {filename}")


def export_clean_json(video_id: str, analyzed: list[dict]):
    os.makedirs(COMMENTS_DIR, exist_ok=True)
    filename = os.path.join(COMMENTS_DIR, f"{video_id}_clean.txt")

    # åªä¿ç•™æœ‰æ„ä¹‰çš„å­—æ®µï¼Œå»æ‰æ•°æ®åº“å†…éƒ¨å­—æ®µ
    clean_data = [
        {
            "comment_id": c["comment_id"],
            "video_id": c["video_id"],
            "parent_id": c["parent_id"],
            "username": c["username"],
            "clean_text": c["clean_text"],
            "like_count": c["like_count"],
            "reply_count": c["reply_count"],
            "created_at": c["created_at"],
            "sentiment_label": c["sentiment_label"],
            "sentiment_score": c["sentiment_score"],
        }
        for c in analyzed
    ]

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(clean_data, f, indent=2, ensure_ascii=False)

    print(f"  å·²å¯¼å‡ºåˆ° {filename}")

# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run():
    init_db()

    # â”€â”€ ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©æ¨¡å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("=" * 60)
    print("  YouTube è¯„è®ºæŠ“å–å·¥å…·")
    print("=" * 60)
    print("  1. æœç´¢å•†å“åç§°ï¼ˆè‡ªåŠ¨æ‰¾ç›¸å…³è§†é¢‘ï¼‰")
    print("  2. ç›´æ¥è¾“å…¥è§†é¢‘é“¾æ¥")
    print("=" * 60)

    mode = input("è¯·é€‰æ‹©æ¨¡å¼ (1/2): ").strip()

    if mode == "1":
        videos = _mode_search()
    elif mode == "2":
        videos = _mode_url()
    else:
        print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
        return

    if not videos:
        print("æœªæ‰¾åˆ°è§†é¢‘ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ– API Key")
        return

    # â”€â”€ ç¬¬äºŒæ­¥ï¼šç¡®è®¤è§†é¢‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\næ‰¾åˆ°ä»¥ä¸‹è§†é¢‘ï¼š")
    print("-" * 60)
    for i, v in enumerate(videos, 1):
        print(f"{i}. {v.description[:55]}")
        print(f"   é¢‘é“: {v.author}")
        print(f"   æ’­æ”¾ {v.view_count:,} | ç‚¹èµ {v.like_count:,} | è¯„è®º {v.comment_count:,}")
        print(f"   https://youtube.com/watch?v={v.video_id}")
    print("-" * 60)

    confirm = input(f"\næ˜¯å¦æŠ“å–ä»¥ä¸Š {len(videos)} ä¸ªè§†é¢‘çš„è¯„è®ºï¼Ÿ(y/n): ").strip().lower()
    if confirm != "y":
        print("å·²å–æ¶ˆ")
        return

    # 3. å¯¹æ¯ä¸ªè§†é¢‘æŠ“å–è¯„è®º + å›å¤ + å¯¼å‡º
    for i, video in enumerate(videos, 1):
        print(f"\n[{i}/{len(videos)}] è·å–è§†é¢‘ä¿¡æ¯...")
        print(f"  æ ‡é¢˜:   {video.description}")
        print(f"  é¢‘é“:   {video.author}")
        print(f"  æ’­æ”¾:   {video.view_count:,}")
        print(f"  è¯„è®ºæ•°: {video.comment_count:,}")
        save_video(video)

        print(f"\nå¼€å§‹æŠ“å–è¯„è®ºå’Œå›å¤...")
        comments = fetch_all_comments(video.video_id)
        save_comments(comments)

        all_stored = get_all_comments(video.video_id)
        top_level  = [c for c in all_stored if c["parent_id"] is None]
        replies    = [c for c in all_stored if c["parent_id"] is not None]

        print(f"\n{'='*50}")
        print(f"æŠ“å–å®Œæˆï¼")
        print(f"  é¡¶å±‚è¯„è®º: {len(top_level)} æ¡")
        print(f"  å›å¤:     {len(replies)} æ¡")
        print(f"  æ€»è®¡:     {len(all_stored)} æ¡")
        print(f"{'='*50}")

        # å‰ 3 æ¡é¢„è§ˆ
        print("\nå‰ 3 æ¡è¯„è®ºé¢„è§ˆï¼ˆå«å›å¤ï¼‰ï¼š")
        count = 0
        for c in all_stored:
            if c["parent_id"] is not None:
                continue
            print(f"\nğŸ’¬ @{c['username']}  ğŸ‘{c['like_count']}")
            print(f"   {c['text'][:80]}{'...' if len(c['text']) > 80 else ''}")
            comment_replies = [r for r in all_stored if r["parent_id"] == c["comment_id"]]
            for r in comment_replies[:2]:
                print(f"   â””â”€ @{r['username']}: {r['text'][:60]}{'...' if len(r['text']) > 60 else ''}")
            if len(comment_replies) > 2:
                print(f"   â””â”€ ... è¿˜æœ‰ {len(comment_replies) - 2} æ¡å›å¤")
            count += 1
            if count >= 3:
                break

        print("\næ­£åœ¨å¯¼å‡º txt æ–‡ä»¶...")
        export_to_txt(video, all_stored)

        all_stored = get_all_comments(video.video_id)

        cleaned = preprocess_comments(all_stored)
        analyzed = analyze_batch(cleaned)
        save_sentiment(analyzed)

        # æ±‡æ€»è¾“å‡º
        summary = get_sentiment_summary(video.video_id)
        all_stored = get_all_comments(video.video_id)
        export_to_txt_v2(video, all_stored, summary)
        export_clean_json(video.video_id, analyzed)
        total = sum(summary.values())
        print(f"\næƒ…æ„Ÿåˆ†æç»“æœï¼š")
        print(f"  æ­£é¢: {summary['positive']} æ¡ ({summary['positive'] / total * 100:.1f}%)")
        print(f"  è´Ÿé¢: {summary['negative']} æ¡ ({summary['negative'] / total * 100:.1f}%)")
        print(f"  ä¸­æ€§: {summary['neutral']}  æ¡ ({summary['neutral'] / total * 100:.1f}%)")

        # æ‰“å°æœ€æ­£é¢ / æœ€è´Ÿé¢å„ 3 æ¡
        positives = get_comments_by_sentiment(video.video_id, "positive")
        negatives = get_comments_by_sentiment(video.video_id, "negative")

        print("\næœ€æ­£é¢çš„ 3 æ¡è¯„è®ºï¼š")
        for r in positives[:3]:
            print(f"  [{r['sentiment_score']:.2f}] {r['clean_text'][:70]}")

        print("\næœ€è´Ÿé¢çš„ 3 æ¡è¯„è®ºï¼š")
        for r in negatives[:3]:
            print(f"  [{r['sentiment_score']:.2f}] {r['clean_text'][:70]}")

if __name__ == "__main__":
    run()